{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I'm Sorry Dave, I Can't Do That...\n",
    "## Adventures in Machine Learning!\n",
    "##### Copyright &copy; 2019, 2020 David Hoelzer / Enclave Forensics Inc., All Rights Reserved\n",
    "> License is granted to copy, use, and reproduce this Notebook and Python code as-is for any non-commercial use.  This license may not be removed.  Any reuse of any portions of the code or text in this Notebook must be attributed with HTTP pointers to the original code.\n",
    "\n",
    "This is a 60-90 minute crash-course excerpt from the new Applied Machine Learning for Information Security course that is in development.  We expect this to be released to the public in the May - June 2020 timeframe.\n",
    "\n",
    "### What We Teach:\n",
    " * What is machine learning and AI today, really (We'll do this tonight *very* quickly\n",
    " * What limitations does ML and AI have today (We'll do this tonight *very, very* quickly\n",
    " * What useful things can I build **now**?  (We'll do this too!!!)\n",
    " \n",
    " ---\n",
    "\n",
    "## What is Machine Learning?\n",
    "### Linear Regression\n",
    "\n",
    "* Take some data points\n",
    "* Work out a function that approximates the data\n",
    "* Hopefully, predict what that data will do in the future\n",
    "\n",
    "There's just one thing we have to make sure you know to provide some context...\n",
    "\n",
    "> # What is this the formula for?\n",
    "> ## y = mx + b\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some quick Python to read in some sample data showing bytes transferred per hour\n",
    "import numpy as np\n",
    "import pandas as p\n",
    "import os\n",
    "import scipy as sp\n",
    "from scipy.stats import gamma\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load Netflow statistics for multiple days\n",
    "data = p.read_csv(\"flowStats.csv\")\n",
    "\n",
    "# Look at it\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning and Processing the Data\n",
    "## Extra Column\n",
    "The extra column is an artifact of the generating tool.  Let's recreate the dataframe, dropping the third column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(data.columns[2], axis=1, inplace=True)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data vs Verification Data\n",
    "In order to know how well our approximation predicts the future, we need to hold back some data so that we can tell how well our result approximates reality or *Ground Truth* using known data that the system hasn't been trained on or fitted to.\n",
    "\n",
    "`bytes` will hold the data we use for fitting or learning.\n",
    "\n",
    "`allbytes` will hold the *ground truth* of all of the known data that we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bytes = data.to_numpy()[0:60,1]\n",
    "allbytes = data.to_numpy()[:,1]\n",
    "# Print the result out so we can see if we got only byte counts.  The result will be in scientific notation by default.\n",
    "bytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hand-wavy Python Magic (Wibbly-Wobbly, Timey-Wimey Stuff)\n",
    "What follows is some Python magic that:\n",
    "\n",
    "* Gets rid of the nasty looking Epoch timestamps, trading them in for hour numbers starting from zero.\n",
    "* Creates a handy function that will graph data for us.\n",
    "* Includes some special Jupyter \"magic\" to display graphs inline in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a set of hours that we can map this against.  Honestly,\n",
    "# we could have simply used the entire data set as-is, but\n",
    "# then we wouldn't have had an excuse to slice an array and\n",
    "# create a range\n",
    "hours = range(0,bytes.size)\n",
    "\n",
    "def plot_data(x, y, models=None, mx=None, ymax=None, fig_idx=None):\n",
    "    '''\n",
    "    Plot the data (y) over time (x). \n",
    "    \n",
    "    models, if present, is an array of polynomial models generated\n",
    "    by polyfit()\n",
    "    '''\n",
    "    x = range(0,x[-1])\n",
    "    plt.figure(figsize=(12,6), dpi=300) # width and height of the plot in inches\n",
    "    plt.scatter(x, y, s=10)\n",
    "    plt.title(\"Bytes per Hour\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Bytes\")\n",
    "    \n",
    "    if models:\n",
    "        # I'm only supporting six plots because that's all that I need. :)\n",
    "        colors = ['g', 'k', 'b', 'm', 'r', 'i']\n",
    "        linestyles = ['-', '-.', '--', '-', '-', '.']\n",
    "\n",
    "        if mx is None:\n",
    "            mx = np.linspace(0, len(x), num=20)\n",
    "        for model, style, color in zip(models, linestyles, colors):\n",
    "            plt.plot(mx, model(mx), linestyle=style, linewidth=2, c=color)\n",
    "\n",
    "        plt.legend([\"d=%i\" % m.order for m in models], loc=\"upper left\")\n",
    "        \n",
    "    plt.autoscale()\n",
    "    if ymax:\n",
    "        plt.ylim(ymax=ymax)\n",
    "\n",
    "    plt.grid()\n",
    "    plt.ylim(bottom=0)\n",
    "\n",
    "# This next line is INCREDIBLY important!  When using iPython and Jupyter,\n",
    "# if you want to get it to plot things inline as you are working with the\n",
    "# notebook, you MUST include this line.  Without it, you will never see\n",
    "# a graph!\n",
    "%matplotlib inline\n",
    "\n",
    "# Let's have a look at the data that we will use for training.\n",
    "plot_data(range(0,len(bytes)+1), bytes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What Now?  Regression!\n",
    "\n",
    "* Regression = \"Fitting\" or \"Approximation\"\n",
    "\n",
    "In other words, can we come up with some function *f* that will produce something really close to those dots?\n",
    "\n",
    "> *(Where **x** is the hour number and **y** is the number of bytes seen)*\n",
    "> Said another more mathy way, can we find *f(x)* where *f(x)* accurately produces a result that matches ground truth?  Moving to this *f(x)* notation, *f(x)* is just a stand-in (or another way of saying) *y*.\n",
    "\n",
    "> Since we are starting with the idea that *y = mx + b*, or *f(x) = mx + b*, we are attempting to find values for *m* and *b* that allow our line to closely match, approximate, or fit reality.\n",
    "\n",
    "### Linear Regression = Draw A Line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need an error function that gives us the average or mean of the squares \n",
    "# of the distances between the actual value and the model.\n",
    "def mean_squared_error(f, x, y):\n",
    "    return np.average((f(x)-y) **2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we try a naive approach of a straight line approximation of our function.\n",
    "# The polyfit function performs this... regression analysis?  We can specify the order of the polynomial, which\n",
    "# is 1 for a line:\n",
    "\n",
    "fp1 = np.polyfit(range(0,len(hours)), bytes, 1)\n",
    "print(f'After fitting for a first order equation, the result is {fp1}')\n",
    "print(f'This translates to:   f(x) = {fp1[0]} * x + ({fp1[1]})')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These means that f(x) = 894781.28905251 * x + -10550937.97704918..., or y=mx + b\n",
    "# To convert that array into a function, we pass it to np.poly1d, where 1d is the order of the function:\n",
    "f1 = np.poly1d(fp1)\n",
    "mean_squared_error(f1, hours, bytes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the same scatter plot and passing f(x)=f1 - Note the square brackets.\n",
    "plot_data(range(0,len(bytes)+1), bytes, [f1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Infinite Series\n",
    "\n",
    "It is (generally) possible for any continuous curve to find a function that will \"fit\" that curve well.  For simple curves this is (relatively) simple.  For more complex curves this is more difficult...  For instance:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sine_range = 10\n",
    "range_to_plot = np.arange(-1 * sine_range,sine_range,0.1)\n",
    "plt.figure(figsize=(12,6), dpi=300)\n",
    "plt.plot(range_to_plot, (np.cos(range_to_plot)))\n",
    "plt.title(f'A Plot of Sine from {-1 * sine_range} pi to {sine_range} pi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The More Terms...\n",
    "\n",
    "We don't want to become too mathy here, but a Taylor series is an infinite series that can be used to approximate the value of Sine or Cosine (and other interesting values) *very* accurately.  *Why and how* this works is very interesting, and I encourage you to investigate the topic if you are curious.  It will give you a much better appreciation of how derivatives can be chained together to control a desired output... but that's not why we're here now.\n",
    "\n",
    "> ### Infinite doesn't have to mean infinity!\n",
    "> It turns out that you can be *very* accurate around a specific point where you need to approximate Sine using very few terms.  The further you venture from that point, however, the worse the approximation becomes...  Said another way, the further you wander away from ground truth...  Or the worse your error is... Which is also called **Loss**.\n",
    "\n",
    "### Let's Try That (in a simpler way)\n",
    "\n",
    "Fundamentally, the more terms we add, the better the approximation.  Adding a term is adding some exponential value of *x*.  Said another way, we are creating a *higher order equation*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def taylor_cosine(x, n=1):\n",
    "    \"\"\"Returns the taylor series approximation of the cosine of X over N terms.\"\"\"\n",
    "#           oo              2n + 1\n",
    "#          ---      n      x\n",
    "#   sin x=  >   (-1)  ------------\n",
    "#          ---         (2n + 1)!\n",
    "#          n=0\n",
    "\n",
    "    approximation = 0\n",
    "    # In Python 3.8+ the following *should* work:\n",
    "    # [approximation := approximation + x for x in ((-1)**i) * ((x**((2*i)+1))/(math.factorial(2*i))) for i in range(n)]\n",
    "    # \n",
    "    # For now, we use a loop:\n",
    "    for i in range(n):\n",
    "        approximation += ((-1)**i) * ((x**((2*i)+1))/(math.factorial(2*i)))\n",
    "    \n",
    "    return approximation\n",
    "\n",
    "taylor_terms_to_calculate = 13\n",
    "\n",
    "plt.figure(figsize=(12,10), dpi=300)\n",
    "plt.plot(range_to_plot, (np.cos(range_to_plot)))\n",
    "plt.ylim(top=4,bottom=-4)\n",
    "for i in range(1,taylor_terms_to_calculate):\n",
    "    plt.plot(range_to_plot, [func_cos(x, i) for x in range_to_plot])\n",
    "\n",
    "plot_legend = ['Cosine function'] + [(f'Taylor Terms: {x}') for x in range(1,taylor_terms_to_calculate) ]\n",
    "\n",
    "plt.legend(plot_legend)\n",
    "plt.title(f'Sine(x) Compared to Taylor Approximations Using Between 1 and {taylor_terms_to_calculate - 1} terms')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wow!\n",
    "Have a look at the graph.  I know it's a lot to take in, but consider what's going on.  Find the Cosine graph and follow it.  Now find the horizontal line that is orange.  That horizontal line is the very first result of a Taylor series with one term.  At *x = 0* it exactly matches the value of *sin(0)*, but after that it only matches the value of *sin(x)* every *2$\\pi$* (or 360$^\\circ$) values !  The dark green graph, a Taylor series that computes two terms looks like it is \"good enough\" between perhaps *-0.5* and *0.5* radians.  If you scan down to the result of using six terms, the values very closely approximate *sin(x)* for a much wider range, nearly *-5.0* to *5.0*.  There are some diminishing returns, however.  The early success at very close approximation seems to decline rapidly.  Doubling the number of terms to 12 we find that the approximation is good for less than twice the range.\n",
    "\n",
    "Even so, this means that we can calculate any arbitrary number of terms we need in order to approximate the value of Sine to any precision.\n",
    "\n",
    "## Wait... What?\n",
    "So what does this have to do with our topic?  Well, so far we've asked Python to come up with a first order equation to approximate our data.  Extending what we see with the Taylor series, it intuitively seems likely that providing a larger number of terms will allow us to fit the data more precisely!  Let's give that a try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clearly this is insufficient.  We can quickly try to fit to other functions by using polyfit to find \n",
    "# coefficients for higher order polynomials:\n",
    "\n",
    "num_hours = range(0,len(bytes))\n",
    "\n",
    "fp2 = sp.polyfit(num_hours, bytes, 2)\n",
    "f2 = sp.poly1d(fp2)\n",
    "fp3 = sp.polyfit(num_hours,bytes, 3)\n",
    "f3 = sp.poly1d(fp3)\n",
    "fp4 = sp.polyfit(num_hours,bytes, 15)\n",
    "f4 = sp.poly1d(fp4)\n",
    "\n",
    "\n",
    "# And let's look at the errors:\n",
    "print(\" 1st Order: \", error(f1, num_hours, bytes))\n",
    "print(\" 2nd Order: \",error(f2, num_hours, bytes))\n",
    "print(\" 4rd Order: \",error(f3, num_hours, bytes))\n",
    "print(\"15th Order: \",error(f4, num_hours, bytes))\n",
    "print(\"\\nClearly the error is getting smaller, so the approximation must be better... Right??\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hours = range(0,len(bytes)+1)\n",
    "plot_data(num_hours, bytes, [f1,f2], mx = np.linspace(num_hours[0], num_hours[-1], 100))\n",
    "plot_data(num_hours, bytes, [f1,f2, f3], mx = np.linspace(num_hours[0], num_hours[-1], 100))\n",
    "plot_data(num_hours, bytes, [f1,f2, f3, f4], mx = np.linspace(num_hours[0], num_hours[-1], 100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Does This Mean?\n",
    "As you can see, the graphs start out looking promising.  The second order equation is better and the third order is even better!  If those are good, a 15th order equation must be amazing!!  But something has seemingly gone awry.\n",
    "\n",
    "Notice that the 15th order equation starts out well, but it is no longer *generalizing* on the data.  Instead, it's almost as though it is darting from point to point, no longer able to \"see\" any overarching patterns in the data.  This is an artifact of overtraining and/or overfitting.  What does this mean?  If we overtrain or overfit our data, we are no longer generalizing.  Instead, our function is *very* good at finding points that are in the known training set, but likely very *bad* at finding or predicting anything else.  We can prove this by graphing out these functions with the hold-out data that we set aside (and we'll do that in a second).\n",
    "\n",
    "Before moving on, recognize that generating a linear regression isn't actually the point here.  There are two much bigger points (or intuitions) that I'd like you to come away with:\n",
    "\n",
    "* Machine Learning (which is how the linear regression was performed) is using an algorithm to adjust values ( coefficients in this case) to minimize the error (or loss) between the prediction (output of the function) and ground-truth (reality)\n",
    "* Overtraining or overfitting will usually result in a model that performs exceptionally well on the *training data* but very poorly on everything else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hours = range(0,len(allbytes)+1)\n",
    "plot_data(num_hours, allbytes, [f1,f2], mx = np.linspace(-10, num_hours[-1]+2, 100))\n",
    "plot_data(num_hours, allbytes, [f1,f2, f3], mx = np.linspace(-10, num_hours[-1]+2, 100))\n",
    "plot_data(num_hours, allbytes, [f1,f2, f3, f4], mx = np.linspace(-10, num_hours[-1]+30, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's quite clear that the overall predictions using regression fitting are pretty poor in this case.  Still, we've gained some valuable insights.\n",
    "\n",
    "---\n",
    "\n",
    "# The Story So Far:\n",
    "\n",
    "* Machine Learning = Automatically adjusting variables to make a prediction\n",
    "* The adjustments are made by minimizing the \"Losses\"\n",
    "* Loss is the same as error\n",
    "* Both of these are how far away from \"Ground Truth\" or reality the predicted value is\n",
    "* Overfitting/overtraining happens when your approximation looks far too closely at the specifics, failing to generalize\n",
    "    * This can also be us doing a bad job of feature selection, which more frequently how the term \"overfitting\" is used.\n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hello, world!\n",
    "### A Quick Detour into Binary Classification\n",
    "\n",
    "> Classifying movie reviews into two categories (binary classification) was a cutting edge problem in natural language and machine learning less than a decade ago.  While we can view this as a \"solved\" problem and there are absolutely better approaches than what we will do here, there are still some very valuable intuitions that we can learn by using this is a bridging example between simple linear regression and more complex logistic regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# We only need a few specific things from Keras, so we import only those elements.\n",
    "from keras import models\n",
    "from keras import layers\n",
    "\n",
    "# Keras.datasets contains a number of interesting/useful datasets, especially when exploring and\n",
    "# experimenting with machine learning.\n",
    "from keras.datasets import imdb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data\n",
    "The IMDB review data is stored as two tuples of tuples.  These are a first tuple representing a set of 25,000 movie reviews (`train_data` in our experiment) and 25,000 classifications (`train_labels` in our experiment).  The second tuple contains a set of another 25,000 movie reviews (`test_data`) along with the matching classifications (`test_labels`).\n",
    "\n",
    "The movie reviews were originally sentences made up of words, but these have been preprocessed to save some time.  The words have been replaced with numbers that represent an index into the set of words used in all reviews.  For example, imagine that we had the following index:\n",
    "\n",
    "1. The\n",
    "2. fox\n",
    "3. brown\n",
    "4. is\n",
    "5. quick\n",
    "6. lazy\n",
    "7. jumps\n",
    "8. dog\n",
    "9. lazy\n",
    "10. jumped\n",
    "11. over\n",
    "\n",
    "Given that index, the following sentence:\n",
    "\n",
    "```\n",
    "the quick brown fox jumps over the lazy dog\n",
    "```\n",
    "\n",
    "would be represented in our dataset as:\n",
    "\n",
    "```\n",
    "1 5 3 2 7 11 1 6 8\n",
    "```\n",
    "\n",
    "The table of words is ordered by frequency, meaning that word with index 1 occurs more frequently than any other word in the dataset.  If a word has an index of 0, this simply means that the word was unique in the data and was not stored.  Having the words organized in this way allows us to limit how many \"features\" (words) our network will have to work with.\n",
    "\n",
    "# Python Magic Time!\n",
    "Let's start by loading the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's define a variable representing the maximum number of words or features that we want\n",
    "# to work with.  This is passed as a parameter to the \"load_data\" function for the imdb dataset\n",
    "# from Keras.\n",
    "\n",
    "WordsToWorkWith = 10000\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=WordsToWorkWith)\n",
    "\n",
    "# Eventually, we will likely want to look at some of the reviews ourselves, so let's load the word\n",
    "# index from the dataset.  This is available with the function call get_word_index()\n",
    "# get_word_index retrieves the table of words\n",
    "word_index = imdb.get_word_index()\n",
    "\n",
    "# Let's have a look at a little bit of the data so far:\n",
    "\n",
    "print(f\"train_data is of shape {train_data.shape} and train_labels is of shape {train_labels.shape}\")\n",
    "print(f\"The first element in train_data is:\\n\\n{train_data[0]}\\n\\n\")\n",
    "print(f\"The first train_label label is: {train_labels[0]}\")\n",
    "firstTenWords = {k: word_index[k] for k in list(word_index)[:10]}\n",
    "print(f\"The word index is a dictionary with {len(word_index.keys())} entries.  Here are the ten key-value pairs in it:\\n{firstTenWords}\")\n",
    "      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making It Readable\n",
    "Right now, the reviews are represented in a way that isn't particularly readable for us, nor is it structured in an ideal way for the path our experiment will take.  In a sense, the data is currently in a sort of in-between form.\n",
    "\n",
    "It would be useful for us to be able to take the series of numbers in the data and turn that back into a set of words that we can read.  Doing so isn't actually difficult, but it will require some quick Python gymnastics because the `word_index` is in a dictionary where the word is the key and the index number is the value.  Why is it stored this way?  Because it makes it very easy to take new reviews and automatically convert them into this same format.  This would allow us to take our trained network and very quickly process new reviews to determine if they are negative reviews (a label value of 0) or positive reviews (a label value of 1).\n",
    "\n",
    "The first review appears to be a positive review, but let's see for ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, we will use some dictionary comprehension features of Python to reverse the keys and values\n",
    "# in the word_index dictionary.  YOU DO NOT NEED TO BE ABLE TO DO THIS!  Remember, this isn't a Python\n",
    "# class. :) . Googling for a \"recipe\" to reverse values in this way is the perfect way to get started.\n",
    "# We've already figured this part out:\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "# Next, we'll create a function that iterates over the values in a sequence of word indices and returns\n",
    "# the matching words in that order.\n",
    "def decodeReview(data, x):\n",
    "    return ' '.join([reverse_word_index.get(i-3, '?') for i in data[x]])\n",
    "\n",
    "# Let's also create a function that returns \"Postive\" or \"Negative\" based on the passed in label\n",
    "def goodOrBad(x):\n",
    "    return 'positive' if x==1 else 'negative'\n",
    "\n",
    "# Now that we've got a function to decode a review, let's look at that review again:\n",
    "review = decodeReview(train_data,0)\n",
    "print(f\"The first review states:\\n\\n{review}\")\n",
    "print(f\"\\n\\nThis was a *{goodOrBad(train_labels[0])}*\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unpleasant Math Interlude\n",
    "\n",
    "Much of machine learning today relies heavily on Linear Algebra.  Don't worry, we're not going to teach you this!  Linear Algebra, put very simply, the algebra of vectors.\n",
    "\n",
    "* A single number is a \"Scalar.\"  We'll explain why this is in a second.\n",
    "* A vector is an ordered set of numbers.  For example *(x, y)* where *x=5* and *(y=1)* would define the coordinate (5,1)\n",
    "    * Yes, I know you physics folks were taught that it has a position and a magnitude, but just go with this math-centric definition!\n",
    "* A matrix is a grid representing many vectors (yes, it could be other things to, but this is good enough).\n",
    "* We can also call these \"Tensors...\"  We'll be back to this.\n",
    "\n",
    "### Vector Math (aka, linear algebra):\n",
    "> What if I multiply our point, *(5,1)*, by 2?  If this coordinate defined the distance in X and Y from the origin of a point, multiplying by 2 would result in (10,2), *scaling* the value...  which is whyt we call them *scalars*\n",
    "\n",
    "---\n",
    "\n",
    "# Ok, that's enough Math...  What's the point\n",
    "\n",
    "### There are very specific rules in this algebra\n",
    "The only important rule that we need to be aware of is that all of our vectors or matrices *must* have the same number of dimensions, otherwise known as \"columns\" or \"coordinates\".\n",
    "\n",
    "#### But Why, Dave?\n",
    "\n",
    "**Because Linear Algebra is all about transforming things from one vector space to another!**\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## With that done, let's create some vectors!\n",
    "* Let's take our movie review and create a vector that has as many columns as we have words in our dictionary.\n",
    "* If a word in our dictionary is present in the review, we flip that column \"On\" (set it to one)\n",
    "* If a word doesn't appear in the review, it is left as zero\n",
    "\n",
    "#### Here's some Python magic that does this for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are very nearly ready to build a network.  First, recall that the \"features\" (data) must\n",
    "# be \"vectorized\".  This means that we need to create a 1xN vector, where N is the number of\n",
    "# features in the dataset.  Our main goal is to make all of the vectors the same dimensions.\n",
    "# Matrix operations, which are at the heart of tensor calculus in machine learning.\n",
    "#\n",
    "# Since our machine learning approach also greatly prefers that all feature values lie between\n",
    "# zero and one, we will take the opportunity to not only vectorize, but to one-hot encode the data!\n",
    "\n",
    "# This is some Python magic that will take the array of reviews, which are sequences of word indices,\n",
    "# and convert every row into a vector that has \"WordsToWorkWith\" dimensions.  If you've followed the\n",
    "# directions so far, you are creating a 10,000 dimensional Tensor.  Wow!\n",
    "# This function additionally puts a 1 at every position within a row that matches the index\n",
    "# number of the words that are found in the review.\n",
    "def vectorize_sequences(sequences, dimension=WordsToWorkWith):\n",
    "    results = np.zeros((len(sequences), dimension))\n",
    "    for i, sequence in enumerate(sequences):\n",
    "        results[i, sequence] = 1.\n",
    "    return results\n",
    "\n",
    "\n",
    "x_train = vectorize_sequences(train_data)\n",
    "x_test = vectorize_sequences(test_data)\n",
    "\n",
    "# WHAT?!?  That's it?  Yes!  But what does it look like... Let's compare:\n",
    "print(f\"Here's what a row ({len(train_data[0])} elements long) from the dataset looks like to start with:\\n\\n{train_data[0]}\\n\")\n",
    "print(f\"Here's what that same row ({x_train[0].shape} dimensions) looks like when it has been vectorized:\\n\\n{x_train[0]}\")\n",
    "\n",
    "# Remember, this encoding is simply marking which words appear in the review\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Bit More Data Manipulation and then Build the Network!\n",
    "\n",
    "We can often do a better job of training if we give our network training data and labels and *validation data and labels*.  Validation data is data used during the training process to check how well the network performs on data that it has not been trained on, feeding that information back into the training process!\n",
    "\n",
    "Let's pull out the validation data and then train our network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we convert the labels from the format that they were in in the dataset to\n",
    "# floating point numpy arrays\n",
    "y_train = np.asarray(train_labels).astype('float32')\n",
    "y_test = np.asarray(test_labels).astype('float32')\n",
    "\n",
    "# Next, we split the training data at element 10,000 so that we have a validation set.\n",
    "# Create a validation set from the training data\n",
    "x_val = x_train[:10000]\n",
    "partial_x_train = x_train[10000:]\n",
    "y_val = y_train[:10000]\n",
    "partial_y_train = y_train[10000:]\n",
    "\n",
    "# Our neural network will have a number of sequential layers\n",
    "model = models.Sequential()\n",
    "# The first layer is a \"Dense\" or \"Fully connected\" layer, meaning that every neuron is connected to every other\n",
    "# neuron in the previous and the next layers.  This layer will arbitrarily gave 16 neurons in it.\n",
    "# Use the Rectified Linear function for activation.  Since this is\n",
    "# the input layer, we must tell it what the shape of the input data is.\n",
    "model.add(layers.Dense(16, activation='relu', input_shape=(WordsToWorkWith,)))\n",
    "# Our model has one hidden layer, which is also a dense, or fully connected, layer.  It arbitrarily has 16 neurons.\n",
    "model.add(layers.Dense(16, activation='relu'))\n",
    "# Our final layer is a dense layer with only one neuron.  Why one neuron?  Because we are really looking for a\n",
    "# binary answer; is this a positive or a negative review?  We apply the sigmoid function to this because the result\n",
    "# is really a percentage between zero and one.\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Next we compile the model\n",
    "model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# and then we fit the model to the data.  This is where the \"Learning\" happens!\n",
    "training_history = model.fit(partial_x_train, partial_y_train, epochs=4, batch_size=512,validation_data=(x_val, y_val))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Neural Network\n",
    "It might seem anticlimactic, but that's it!  You have now trained a neural network, which should have an accuracy greater than 90%, that is able to classify IMDB movie reviews as positive or negative!\n",
    "\n",
    "How do we use it?  Let's hand our test data to the network, have it classify all 25,000 of those reviews, and see what we think of the output!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We've made this a separate step just so that you can appreciate what we have created.  While the training didn't\n",
    "# take a very long time, some networks can take days, weeks, or more to train!  Still, using the network after\n",
    "# training is typically very fast!\n",
    "predictions = model.predict(x_test)\n",
    "# That's it!  The model has been applied and the predictions have been made!  Let's look at the first five reviews \n",
    "# from the test data:\n",
    "\n",
    "for i in range(0,5):\n",
    "    goodBad = \"Negative\" if predictions[i] < 0.5  else \"Positive\"\n",
    "    print(\"%s review: [%d%%]\"%(goodBad, predictions[i]*100))\n",
    "    print(decodeReview(test_data, i))\n",
    "    print(\"------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important Takeaways\n",
    "There are some super important intuitions that you should have developed from this latest discussion:\n",
    "\n",
    "* Look closely at the predictions, expecially the fourth prediction just above.  It is quite likely that your network has mis-classified this as a Positive review even though it is obviously negative.  *Machine learning algorithms **simulate** intelligence.  They do not actually **understand** anything.*\n",
    "* This is really all just math.  A Neural Network, like the one above, is essentially just working out probabilities.  This is how all (most?) classification problems work, regardless of whether there are only two categories (binary) or many categories.\n",
    "* Massaging the data into a format that can be used in a machine learning algorithm definitely takes some effort.\n",
    "* When we structure the data that gets presented to the algorithm, all of the data must have the same number of dimensions.\n",
    "    * These dimensions are just the number of terms in the vector (or ordered list of values).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
